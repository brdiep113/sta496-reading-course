---
output:
  pdf_document: default
  html_document: default
---
### Language Models are Fewshot Learners
- pre-trained language models like GPT-3 can be made to perform relatively well on new language tasks which it was not specifically trained on with relatively few samples
- zero-shot, one-shot, and few-shot refer to how many examples the model is given before being asked to repeat the task on a new sample
- GPT-3's performance when doing one or few-shot learning on a variety of tasks ranging from machine translation, fill-in-the-blanks, reading comprehension, natural language inference, and reasoning, are all able to approach SOTA or otherwise provide a solid baseline of purpose-built models!
- GPT-3 still however is not completetly indistinguishable from human-generated text and it can easily be tripped up in reasoning problems where a human would not have any troubles
- GPT-3 is also trained on a level of data that is orders of magnitude larger than anything a human would see in their life time which suggests that it is not efficiently using this data
- few-shot learning is also questionable in that it is unclear whether or not a model is truly learning this new task on the fly or if it is simply recognizing the context that the learning queries are made and finding similar contexts in the training corpus
- knowing the performacne of GPT-3 also suggests that it can be used in malicious contexts
- the model also has some biases that it has learned from the data with gender and race being particularly sensitive

### On the Lack of Robust Interpretability of Neural Text Classifiers
- interpretability is increasingly important for understanding neural language models
- feature-based interpretability is one of the most common points of view that we have taken in interpreting language models
- this refers to how the features used as input, and their impact on the model output
- however many of these feature-based interpretability metrics have not been measured at length for their actual robustness
- some of these tests are:
  - Different Initialization Test
    - measures the feature attributions between two models that are trained identically in every aspect except for their randomly chosen initial parameters
    - if the predictions generated by the model are identical, then we expect the feature attributions to be the same for functionally equivalent models
    - if the attributions are not the same, then the same input may suggest that the features have different importances
  - Untrained Model Test
    - given an input, compare the feature attributiosn generated on a fully trained model compared to one with a randomly initialized untrained model
    - evaluate whether the feature attributions on the fully trained model differ from the untrained one as expected
  - Measures for feature attribution:
    - Fidelity
      - get a vector of feature attributions of the corresponding tokens using the interpretability method to be evaluated
      - drop features in decreasing order of score until the model prediction changes
      - infidelity is the % of features that need to be dropped until the prediction changes
    - Jaccard similarity
      - measure the similarity between the top K % highest feature-attribution scores under two methods
  - in their tests they find that the fidelity of interpretations is reasonably high in untrained models (where we suggest the fidelity to be very low), then it may not be very useful to rely on these interpretations
  - this is non-robust behaviour that is not desirable for interpretability metrics
  
### The messy, secretive reality behind OpenAI's bid to save the world
- OpenAI is a leading AI research lab with the goal of creating artificial general intelligence
- the mission of the lab is to develop the technology in a way such that it is developed ethically and distributed for the good of society
- however OpenAI's culture seems to run at odds with this stated goal as it has failed to stay true to its stated values of transparency, openness, and collaboration
- OpenAI is funded by investors but they began as a non-profit with the goal of being a more transparent organization than other similar AI research labs at the time which were firmly privatized
- nonetheless conflict between it's reliance on shareholders and investors and it's core ethos/team has lead to a shift away from the purely altruistic stated goal
- OpenAI's staged release of GPT-2 was controversial but reflected an understanding of its ethical obligations
- the GPT-2 fiasco was related to other opportunities where OpenAI chose not to be transparent about the research it was doing 
- diversity and representation within OpenAI's upper leadership is also a point of criticism as women and ethnic minorities are underrepresented
- OpenAI here acts as a case-study about the conflict between developing AI ethically and how it runs up against the expectations of technology when it is a vehicle for investment

