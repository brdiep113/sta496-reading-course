---
output:
  pdf_document: default
  html_document: default
---
### On learning and representing social meaning in NLP: a sociolinguistic perspective
- NLP has the potential to inform work in sociolinguistics and modelling social meaning instead of merely semantic meaning
- social meaning refers to how language is used to communicate not only the actual content of the message, but also the social identity and pragmatic factors that surround an interaction
- spelling and phonetic variation, regional slang and registers of language are all things that can be covered under sociolinguistics and social meaning
- social meaning can be attached to different linguistic forms
- linguistic variation can be indicative of identity
- social meaning is very contextual and suggests that we need stronger contextual representations than word-embeddings have typically used
- this work would be useful for conversational systems that can be both more consistent in emulating a persona or speaker style, as well as adjusting based on feedback from a user
- understanding social meaning of language in NLP can also help with bullying or abusive content detection 
- NLP also opens up exciting avenues for traditional sociolinguistics research by applying computational methods to study larger corpora

### BERT
- BERT is a SOTA language representation model that is designed to create deep bi-directional representations from unlabelled text
- bidirectionality is important as we reasonably assume that context of both the word preceding and following a token are important for understanding the word
- BERT is trained in an un-supervised way using two tasks
  - masked language modeling task, where some percentage of input tokens are randomly masked and the model is trained to predict these tokens
  - next sentence prediction task, where the model is asked to predict a binary task of whether a given sentence follows the current one
- after training on these tasks, BERT can then be used on a variety of down-stream tasks
- BERT is a transformer-based architecture that leverages self-attention
- BERT can create these contextually-bound token embeddings which takes into account the context, which is unique from other word-embedding methodologies
- BERT in the time since it's release has practically become the SOTA for language tasks and forms the baseline of many more specifically fine-tuned models like RoBERTa or SBERT

